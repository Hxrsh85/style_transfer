{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d0c0fb9-b59b-46b8-8b73-71e898759ded",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b44ecc76-c7cb-482c-ab28-76cbc89774ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ece340-e48e-4359-8ac5-4461f317ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.preprocessing import image as kimage\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0fbc25f-8c07-4c89-b213-ca5a92c2be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = tf.device(\"GPU:0\" if tf.config.experimental.list_physical_devices('GPU') else \"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76009a2-8348-4f1c-85d3-7e83b5008cc4",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ffb7fb1-f25d-4587-9834-8a92f10b5119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG19(weights='imagenet', include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "layer_names = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block4_conv2', 'block5_conv1']\n",
    "outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a95f13a-d908-406a-8363-d966c1f57bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_activations(input_img, model):\n",
    "    activations = model(input_img)\n",
    "    features = {layer_names[i]: activations[i] for i in range(len(layer_names))}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0033182d-ea84-4ee6-9418-118670015084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = kimage.load_img(image_path, target_size=(300, 300))\n",
    "    img = kimage.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(img):\n",
    "    img = img.reshape((300, 300, 3))\n",
    "    img[:, :, 0] += 103.939\n",
    "    img[:, :, 1] += 116.779\n",
    "    img[:, :, 2] += 123.68\n",
    "    img = img[:, :, ::-1]\n",
    "    img = np.clip(img, 0, 255).astype('uint8')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ac82a32-b2b8-4585-9f1b-7edc00290d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content shape: (1, 300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "content_image = preprocess_image(\"/Users/harshsingh/Desktop/Style transfer/Dataset/monaLisa.jpg\")\n",
    "style_image = preprocess_image(\"/Users/harshsingh/Desktop/Style transfer/Dataset/MonaLisaStyle.jpg\")\n",
    "\n",
    "print(\"Content shape:\", content_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6334413f-4091-4ffe-a036-c7d0cffd1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(img_features):\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', img_features, img_features)\n",
    "    input_shape = tf.shape(img_features)\n",
    "    num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n",
    "    return result / num_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5084decf-93a3-42ae-af53-dbd44c328ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_weights = {\"block1_conv1\": 1.0, \n",
    "                 \"block2_conv1\": 0.8,\n",
    "                 \"block3_conv1\": 0.4,\n",
    "                 \"block4_conv1\": 0.2,\n",
    "                 \"block5_conv1\": 0.1}\n",
    "\n",
    "content_weight = 100\n",
    "style_weight = 1e8\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbed73c4-a294-40ac-94c9-03414b5bf764",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/ipykernel_2021/1067025086.py\", line 26, in train_step  *\n        loss = compute_loss(target)\n    File \"/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/ipykernel_2021/1067025086.py\", line 17, in compute_loss  *\n        _, d, h, w = target_gram.shape\n\n    ValueError: not enough values to unpack (expected 4, got 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m print_after \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 35\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_step(target_image)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/__autograph_generated_fileiupp_5wg.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(compute_loss), (ag__\u001b[38;5;241m.\u001b[39mld(target),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     12\u001b[0m grad \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(target)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, ([(ag__\u001b[38;5;241m.\u001b[39mld(grad), ag__\u001b[38;5;241m.\u001b[39mld(target))],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/__autograph_generated_file_upw_6gt.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     34\u001b[0m layer \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m style_gram \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle_gram\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m ag__\u001b[38;5;241m.\u001b[39mfor_stmt(ag__\u001b[38;5;241m.\u001b[39mld(style_weights), \u001b[38;5;28;01mNone\u001b[39;00m, loop_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,), {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterate_names\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     37\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(content_weight) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(content_loss) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(style_weight) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(style_loss)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/__autograph_generated_file_upw_6gt.py:26\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     24\u001b[0m style_gram \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(style_grams)[ag__\u001b[38;5;241m.\u001b[39mld(layer)]\n\u001b[1;32m     25\u001b[0m target_gram \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(gram_matrix), (ag__\u001b[38;5;241m.\u001b[39mld(target_features)[ag__\u001b[38;5;241m.\u001b[39mld(layer)],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 26\u001b[0m _, d, h, w \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(target_gram)\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     27\u001b[0m style_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(style_loss)\n\u001b[1;32m     28\u001b[0m style_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m style_weights[layer] \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(tf\u001b[38;5;241m.\u001b[39mreduce_mean, ((target_gram \u001b[38;5;241m-\u001b[39m style_gram) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m/\u001b[39m (d \u001b[38;5;241m*\u001b[39m h \u001b[38;5;241m*\u001b[39m w)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/ipykernel_2021/1067025086.py\", line 26, in train_step  *\n        loss = compute_loss(target)\n    File \"/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/ipykernel_2021/1067025086.py\", line 17, in compute_loss  *\n        _, d, h, w = target_gram.shape\n\n    ValueError: not enough values to unpack (expected 4, got 3)\n"
     ]
    }
   ],
   "source": [
    "content_features = model_activations(content_image, model)\n",
    "style_features = model_activations(style_image, model)\n",
    "\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "target_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "\n",
    "@tf.function\n",
    "def compute_loss(target):\n",
    "    target_features = model_activations(target, model)\n",
    "    content_loss = tf.reduce_mean((content_features['block4_conv2'] - target_features['block4_conv2']) ** 2)\n",
    "    \n",
    "    style_loss = 0\n",
    "    for layer in style_weights:\n",
    "        style_gram = style_grams[layer]\n",
    "        target_gram = gram_matrix(target_features[layer])\n",
    "        _, d, h, w = target_gram.shape\n",
    "        style_loss += style_weights[layer] * tf.reduce_mean((target_gram - style_gram) ** 2) / (d * h * w)\n",
    "    \n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    return total_loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(target)\n",
    "    grad = tape.gradient(loss, target)\n",
    "    optimizer.apply_gradients([(grad, target)])\n",
    "    return loss\n",
    "\n",
    "epochs = 4000\n",
    "print_after = 500\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "    loss = train_step(target_image)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Epoch {i}, Loss: {loss.numpy()}\")\n",
    "    \n",
    "    if i % print_after == 0:\n",
    "        img = deprocess_image(target_image.numpy())\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Epoch {i}\")\n",
    "        plt.show()\n",
    "        plt.imsave(f'{i}.png', img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a329450-875c-4569-b5d7-dcc8960358a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0480838-0e4d-41ae-ab9b-d01d59b77839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content shape: (1, 300, 300, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/ipykernel_2021/2443125723.py\", line 92, in train_step  *\n        loss = compute_loss(target)\n    File \"/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/ipykernel_2021/2443125723.py\", line 83, in compute_loss  *\n        _, d, h, w = target_shape[0], target_shape[1], target_shape[2], target_shape[3]\n\n    ValueError: slice index 3 of dimension 0 out of bounds. for '{{node strided_slice_5}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape_1, strided_slice_5/stack, strided_slice_5/stack_1, strided_slice_5/stack_2)' with input shapes: [3], [1], [1], [1] and with computed input tensors: input[1] = <3>, input[2] = <4>, input[3] = <1>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m print_after \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 101\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_step(target_image)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/__autograph_generated_file6ot0mgya.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(compute_loss), (ag__\u001b[38;5;241m.\u001b[39mld(target),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     12\u001b[0m grad \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(target)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, ([(ag__\u001b[38;5;241m.\u001b[39mld(grad), ag__\u001b[38;5;241m.\u001b[39mld(target))],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/__autograph_generated_file0926upk8.py:38\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     36\u001b[0m h \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m layer \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m ag__\u001b[38;5;241m.\u001b[39mfor_stmt(ag__\u001b[38;5;241m.\u001b[39mld(style_weights), \u001b[38;5;28;01mNone\u001b[39;00m, loop_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,), {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterate_names\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     39\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(content_weight) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(content_loss) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(style_weight) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(style_loss)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/__autograph_generated_file0926upk8.py:27\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     25\u001b[0m target_gram \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(gram_matrix), (ag__\u001b[38;5;241m.\u001b[39mld(target_features)[ag__\u001b[38;5;241m.\u001b[39mld(layer)],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     26\u001b[0m target_shape \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(target_gram),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 27\u001b[0m _, d, h, w \u001b[38;5;241m=\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mld(target_shape)[\u001b[38;5;241m0\u001b[39m], ag__\u001b[38;5;241m.\u001b[39mld(target_shape)[\u001b[38;5;241m1\u001b[39m], ag__\u001b[38;5;241m.\u001b[39mld(target_shape)[\u001b[38;5;241m2\u001b[39m], ag__\u001b[38;5;241m.\u001b[39mld(target_shape)[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     28\u001b[0m style_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(style_loss)\n\u001b[1;32m     29\u001b[0m style_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m style_weights[layer] \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(tf\u001b[38;5;241m.\u001b[39mreduce_mean, ((target_gram \u001b[38;5;241m-\u001b[39m style_gram) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m/\u001b[39m (d \u001b[38;5;241m*\u001b[39m h \u001b[38;5;241m*\u001b[39m w)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/ipykernel_2021/2443125723.py\", line 92, in train_step  *\n        loss = compute_loss(target)\n    File \"/var/folders/4k/g_jvzmx51w5cq6jkh2ybcynh0000gn/T/ipykernel_2021/2443125723.py\", line 83, in compute_loss  *\n        _, d, h, w = target_shape[0], target_shape[1], target_shape[2], target_shape[3]\n\n    ValueError: slice index 3 of dimension 0 out of bounds. for '{{node strided_slice_5}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape_1, strided_slice_5/stack, strided_slice_5/stack_1, strided_slice_5/stack_2)' with input shapes: [3], [1], [1], [1] and with computed input tensors: input[1] = <3>, input[2] = <4>, input[3] = <1>.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.preprocessing import image as kimage\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "device = tf.device(\"GPU:0\" if tf.config.experimental.list_physical_devices('GPU') else \"CPU\")\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = kimage.load_img(image_path, target_size=(300, 300))\n",
    "    img = kimage.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(img):\n",
    "    img = img.reshape((300, 300, 3))\n",
    "    img[:, :, 0] += 103.939\n",
    "    img[:, :, 1] += 116.779\n",
    "    img[:, :, 2] += 123.68\n",
    "    img = img[:, :, ::-1]\n",
    "    img = np.clip(img, 0, 255).astype('uint8')\n",
    "    return img\n",
    "\n",
    "def save_image(img, filename):\n",
    "    img = deprocess_image(img)\n",
    "    plt.imsave(filename, img)\n",
    "\n",
    "base_model = VGG19(weights='imagenet', include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "layer_names = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block4_conv2', 'block5_conv1']\n",
    "outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "def model_activations(input_img, model):\n",
    "    activations = model(input_img)\n",
    "    features = {layer_names[i]: activations[i] for i in range(len(layer_names))}\n",
    "    return features\n",
    "\n",
    "def gram_matrix(img_features):\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', img_features, img_features)\n",
    "    input_shape = tf.shape(img_features)\n",
    "    num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n",
    "    return result / num_locations\n",
    "\n",
    "content_image = preprocess_image(\"/Users/harshsingh/Desktop/Style transfer/Dataset/monaLisa.jpg\")\n",
    "style_image = preprocess_image(\"/Users/harshsingh/Desktop/Style transfer/Dataset/MonaLisaStyle.jpg\")\n",
    "\n",
    "print(\"Content shape:\", content_image.shape)\n",
    "\n",
    "style_weights = {\"block1_conv1\": 1.0, \n",
    "                 \"block2_conv1\": 0.8,\n",
    "                 \"block3_conv1\": 0.4,\n",
    "                 \"block4_conv1\": 0.2,\n",
    "                 \"block5_conv1\": 0.1}\n",
    "\n",
    "content_weight = 100\n",
    "style_weight = 1e8\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.007)\n",
    "\n",
    "content_features = model_activations(content_image, model)\n",
    "style_features = model_activations(style_image, model)\n",
    "\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "target_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "\n",
    "@tf.function\n",
    "def compute_loss(target):\n",
    "    target_features = model_activations(target, model)\n",
    "    content_loss = tf.reduce_mean((content_features['block4_conv2'] - target_features['block4_conv2']) ** 2)\n",
    "    \n",
    "    style_loss = 0\n",
    "    for layer in style_weights:\n",
    "        style_gram = style_grams[layer]\n",
    "        target_gram = gram_matrix(target_features[layer])\n",
    "        target_shape = tf.shape(target_gram)\n",
    "        _, d, h, w = target_shape[0], target_shape[1], target_shape[2], target_shape[3]\n",
    "        style_loss += style_weights[layer] * tf.reduce_mean((target_gram - style_gram) ** 2) / (d * h * w)\n",
    "    \n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    return total_loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(target)\n",
    "    grad = tape.gradient(loss, target)\n",
    "    optimizer.apply_gradients([(grad, target)])\n",
    "    return loss\n",
    "\n",
    "epochs = 4000\n",
    "print_after = 500\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "    loss = train_step(target_image)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Epoch {i}, Loss: {loss.numpy()}\")\n",
    "    \n",
    "    if i % print_after == 0:\n",
    "        img = target_image.numpy()\n",
    "        save_image(img, f'{i}.png')\n",
    "        plt.imshow(deprocess_image(img))\n",
    "        plt.title(f\"Epoch {i}\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "270c7615-8a3f-4f27-9e89-a8af25a66ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d1abcf0-f5f2-4e5c-b5af-f9563d0e33aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
    "    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if max_size:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "    \n",
    "    if shape:\n",
    "        image = image.resize(shape, Image.LANCZOS)\n",
    "    \n",
    "    if transform:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17fa20d7-341d-4b34-b2df-de5b86628a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28'] \n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff7e8016-3bbc-4a73-bfa3-b3888c9324fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    \n",
    "    # Image preprocessing\n",
    "    # VGGNet was trained on ImageNet where images are normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
    "    # We use the same normalization statistics here.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Load content and style images\n",
    "    # Make the style image same size as the content image\n",
    "    content = load_image('/Users/harshsingh/Desktop/Style transfer/Dataset/monaLisa.jpg', transform, max_size=config.max_size)\n",
    "    style = load_image('/Users/harshsingh/Desktop/Style transfer/Dataset/MonaLisaStyle.jpg', transform, shape=[content.size(2), content.size(3)])\n",
    "    \n",
    "    # Initialize a target image with the content image\n",
    "    target = content.clone().requires_grad_(True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([target], lr=config.lr, betas=[0.5, 0.999])\n",
    "    vgg = VGGNet().to(device).eval()\n",
    "    \n",
    "    for step in range(config.total_step):\n",
    "        \n",
    "        # Extract multiple(5) conv feature vectors\n",
    "        target_features = vgg(target)\n",
    "        content_features = vgg(content)\n",
    "        style_features = vgg(style)\n",
    "\n",
    "        style_loss = 0\n",
    "        content_loss = 0\n",
    "        for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "            # Compute content loss with target and content images\n",
    "            content_loss += torch.mean((f1 - f2)**2)\n",
    "\n",
    "            # Reshape convolutional feature maps\n",
    "            _, c, h, w = f1.size()\n",
    "            f1 = f1.view(c, h * w)\n",
    "            f3 = f3.view(c, h * w)\n",
    "\n",
    "            # Compute gram matrix\n",
    "            f1 = torch.mm(f1, f1.t())\n",
    "            f3 = torch.mm(f3, f3.t())\n",
    "\n",
    "            # Compute style loss with target and style images\n",
    "            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n",
    "        \n",
    "        # Compute total loss, backprop and optimize\n",
    "        loss = content_loss + config.style_weight * style_loss \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step+1) % config.log_step == 0:\n",
    "            print ('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}' \n",
    "                   .format(step+1, config.total_step, content_loss.item(), style_loss.item()))\n",
    "\n",
    "        if (step+1) % config.sample_step == 0:\n",
    "            # Save the generated image\n",
    "            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "            img = target.clone().squeeze()\n",
    "            img = denorm(img).clamp_(0, 1)\n",
    "            torchvision.utils.save_image(img, 'output-{}.png'.format(step+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7d9ed-df5c-48e1-8d27-d74bbf588938",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'content_path': '/path/to/content/image.jpg',\n",
    "    'style_path': '/path/to/style/image.jpg',\n",
    "    'output_dir': './output',\n",
    "    'max_size': 400,\n",
    "    'total_step': 2000,\n",
    "    'log_step': 10,\n",
    "    'sample_step': 500,\n",
    "    'style_weight': 100,\n",
    "    'lr': 0.003\n",
    "}\n",
    "\n",
    "# Create output directory if not exists\n",
    "if not os.path.exists(config['output_dir']):\n",
    "    os.makedirs(config['output_dir'])\n",
    "\n",
    "# Run the main function\n",
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "449421fd-20ea-4b54-89cf-5b8e88a7e09c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'PIL.Image' has no attribute 'ANTIALIAS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m         lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.003\u001b[39m\n\u001b[1;32m     14\u001b[0m     config \u001b[38;5;241m=\u001b[39m Config()\n\u001b[0;32m---> 15\u001b[0m     main(config)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Running as a standalone script\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     parser \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser()\n",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      6\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      7\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      8\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m), \n\u001b[1;32m      9\u001b[0m                          std\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m))])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load content and style images\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Make the style image same size as the content image\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m content \u001b[38;5;241m=\u001b[39m load_image(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/harshsingh/Desktop/Style transfer/Dataset/monaLisa.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, transform, max_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_size)\n\u001b[1;32m     14\u001b[0m style \u001b[38;5;241m=\u001b[39m load_image(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/harshsingh/Desktop/Style transfer/Dataset/MonaLisaStyle.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, transform, shape\u001b[38;5;241m=\u001b[39m[content\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m), content\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m)])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize a target image with the content image\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(image_path, transform, max_size, shape)\u001b[0m\n\u001b[1;32m      8\u001b[0m     scale \u001b[38;5;241m=\u001b[39m max_size \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(image\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m      9\u001b[0m     size \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image\u001b[38;5;241m.\u001b[39msize) \u001b[38;5;241m*\u001b[39m scale\n\u001b[0;32m---> 10\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mresize(size\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), Image\u001b[38;5;241m.\u001b[39mANTIALIAS)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape:\n\u001b[1;32m     13\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mresize(shape, Image\u001b[38;5;241m.\u001b[39mLANCZOS)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'PIL.Image' has no attribute 'ANTIALIAS'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--content', type=str, default='png/content.png')\n",
    "    parser.add_argument('--style', type=str, default='png/style.png')\n",
    "    parser.add_argument('--max_size', type=int, default=400)\n",
    "    parser.add_argument('--total_step', type=int, default=2000)\n",
    "    parser.add_argument('--log_step', type=int, default=10)\n",
    "    parser.add_argument('--sample_step', type=int, default=500)\n",
    "    parser.add_argument('--style_weight', type=float, default=100)\n",
    "    parser.add_argument('--lr', type=float, default=0.003)\n",
    "    config = parser.parse_args()\n",
    "    print(config)\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f484c5-3e8a-474d-bda6-67ac67a7311a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6f13c-6620-4bd7-a5d4-1335938eeaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c8f222-27f5-48ab-b3a7-a61745591b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08e7a0-0f97-44af-a996-35f1c97fe76d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327882f-9125-463f-a15d-f535bea8e1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/2000], Content Loss: 3.7166, Style Loss: 508.5862\n",
      "Step [20/2000], Content Loss: 8.8290, Style Loss: 348.2332\n",
      "Step [30/2000], Content Loss: 13.0189, Style Loss: 262.6015\n",
      "Step [40/2000], Content Loss: 16.4286, Style Loss: 209.0936\n",
      "Step [50/2000], Content Loss: 19.1647, Style Loss: 172.7243\n",
      "Step [60/2000], Content Loss: 21.4725, Style Loss: 146.3952\n",
      "Step [70/2000], Content Loss: 23.4462, Style Loss: 126.6476\n",
      "Step [80/2000], Content Loss: 25.1690, Style Loss: 111.4013\n",
      "Step [90/2000], Content Loss: 26.6881, Style Loss: 99.3812\n",
      "Step [100/2000], Content Loss: 28.0319, Style Loss: 89.7030\n",
      "Step [110/2000], Content Loss: 29.2331, Style Loss: 81.7800\n",
      "Step [120/2000], Content Loss: 30.3219, Style Loss: 75.1866\n",
      "Step [130/2000], Content Loss: 31.2947, Style Loss: 69.6356\n",
      "Step [140/2000], Content Loss: 32.1707, Style Loss: 64.8970\n",
      "Step [150/2000], Content Loss: 32.9644, Style Loss: 60.8053\n",
      "Step [160/2000], Content Loss: 33.6949, Style Loss: 57.2299\n",
      "Step [170/2000], Content Loss: 34.3635, Style Loss: 54.0744\n",
      "Step [180/2000], Content Loss: 34.9767, Style Loss: 51.2709\n",
      "Step [190/2000], Content Loss: 35.5425, Style Loss: 48.7608\n",
      "Step [200/2000], Content Loss: 36.0672, Style Loss: 46.4976\n",
      "Step [210/2000], Content Loss: 36.5539, Style Loss: 44.4455\n",
      "Step [220/2000], Content Loss: 37.0066, Style Loss: 42.5726\n",
      "Step [230/2000], Content Loss: 37.4262, Style Loss: 40.8553\n",
      "Step [240/2000], Content Loss: 37.8143, Style Loss: 39.2731\n",
      "Step [250/2000], Content Loss: 38.1759, Style Loss: 37.8090\n",
      "Step [260/2000], Content Loss: 38.5151, Style Loss: 36.4499\n",
      "Step [270/2000], Content Loss: 38.8329, Style Loss: 35.1822\n",
      "Step [280/2000], Content Loss: 39.1334, Style Loss: 33.9975\n",
      "Step [290/2000], Content Loss: 39.4191, Style Loss: 32.8871\n",
      "Step [300/2000], Content Loss: 39.6891, Style Loss: 31.8436\n",
      "Step [310/2000], Content Loss: 39.9467, Style Loss: 30.8608\n",
      "Step [320/2000], Content Loss: 40.1925, Style Loss: 29.9312\n",
      "Step [330/2000], Content Loss: 40.4282, Style Loss: 29.0514\n",
      "Step [340/2000], Content Loss: 40.6544, Style Loss: 28.2183\n",
      "Step [350/2000], Content Loss: 40.8721, Style Loss: 27.4281\n",
      "Step [360/2000], Content Loss: 41.0819, Style Loss: 26.6783\n",
      "Step [370/2000], Content Loss: 41.2852, Style Loss: 25.9650\n",
      "Step [380/2000], Content Loss: 41.4815, Style Loss: 25.2856\n",
      "Step [390/2000], Content Loss: 41.6710, Style Loss: 24.6387\n",
      "Step [400/2000], Content Loss: 41.8531, Style Loss: 24.0216\n",
      "Step [410/2000], Content Loss: 42.0299, Style Loss: 23.4323\n",
      "Step [420/2000], Content Loss: 42.2005, Style Loss: 22.8684\n",
      "Step [430/2000], Content Loss: 42.3662, Style Loss: 22.3288\n",
      "Step [440/2000], Content Loss: 42.5265, Style Loss: 21.8126\n",
      "Step [450/2000], Content Loss: 42.6831, Style Loss: 21.3178\n",
      "Step [460/2000], Content Loss: 42.8353, Style Loss: 20.8435\n",
      "Step [470/2000], Content Loss: 42.9830, Style Loss: 20.3886\n",
      "Step [480/2000], Content Loss: 43.1255, Style Loss: 19.9519\n",
      "Step [490/2000], Content Loss: 43.2627, Style Loss: 19.5330\n",
      "Step [500/2000], Content Loss: 43.3980, Style Loss: 19.1311\n",
      "Step [510/2000], Content Loss: 43.5311, Style Loss: 18.7449\n",
      "Step [520/2000], Content Loss: 43.6615, Style Loss: 18.3733\n",
      "Step [530/2000], Content Loss: 43.7902, Style Loss: 18.0154\n",
      "Step [540/2000], Content Loss: 43.9166, Style Loss: 17.6710\n",
      "Step [550/2000], Content Loss: 44.0394, Style Loss: 17.3397\n",
      "Step [560/2000], Content Loss: 44.1588, Style Loss: 17.0205\n",
      "Step [570/2000], Content Loss: 44.2776, Style Loss: 16.7128\n",
      "Step [580/2000], Content Loss: 44.3950, Style Loss: 16.4164\n",
      "Step [590/2000], Content Loss: 44.5104, Style Loss: 16.1308\n",
      "Step [600/2000], Content Loss: 44.6238, Style Loss: 15.8551\n",
      "Step [610/2000], Content Loss: 44.7354, Style Loss: 15.5892\n",
      "Step [620/2000], Content Loss: 44.8448, Style Loss: 15.3326\n",
      "Step [630/2000], Content Loss: 44.9520, Style Loss: 15.0848\n",
      "Step [640/2000], Content Loss: 45.0578, Style Loss: 14.8456\n",
      "Step [650/2000], Content Loss: 45.1611, Style Loss: 14.6148\n",
      "Step [660/2000], Content Loss: 45.2621, Style Loss: 14.3920\n",
      "Step [670/2000], Content Loss: 45.3610, Style Loss: 14.1768\n",
      "Step [680/2000], Content Loss: 45.4584, Style Loss: 13.9686\n",
      "Step [690/2000], Content Loss: 45.5539, Style Loss: 13.7670\n",
      "Step [700/2000], Content Loss: 45.6481, Style Loss: 13.5716\n",
      "Step [710/2000], Content Loss: 45.7407, Style Loss: 13.3823\n",
      "Step [720/2000], Content Loss: 45.8319, Style Loss: 13.1989\n",
      "Step [730/2000], Content Loss: 45.9221, Style Loss: 13.0210\n",
      "Step [740/2000], Content Loss: 46.0105, Style Loss: 12.8486\n",
      "Step [750/2000], Content Loss: 46.0976, Style Loss: 12.6811\n",
      "Step [760/2000], Content Loss: 46.1835, Style Loss: 12.5186\n",
      "Step [770/2000], Content Loss: 46.2680, Style Loss: 12.3608\n",
      "Step [780/2000], Content Loss: 46.3510, Style Loss: 12.2074\n",
      "Step [790/2000], Content Loss: 46.4328, Style Loss: 12.0581\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
    "    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if max_size:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.LANCZOS)\n",
    "    \n",
    "    if shape:\n",
    "        image = image.resize(shape, Image.LANCZOS)\n",
    "    \n",
    "    if transform:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image.to(device)\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28'] \n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "def main(config):\n",
    "    \n",
    "    # Image preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Load content and style images\n",
    "    # Make the style image the same size as the content image\n",
    "    content = load_image(config['content_path'], transform, max_size=config['max_size'])\n",
    "    style = load_image(config['style_path'], transform, shape=[content.size(2), content.size(3)])\n",
    "    \n",
    "    # Initialize a target image with the content image\n",
    "    target = content.clone().requires_grad_(True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([target], lr=config['lr'], betas=[0.5, 0.999])\n",
    "    vgg = VGGNet().to(device).eval()\n",
    "    \n",
    "    for step in range(config['total_step']):\n",
    "        \n",
    "        # Extract multiple(5) conv feature vectors\n",
    "        target_features = vgg(target)\n",
    "        content_features = vgg(content)\n",
    "        style_features = vgg(style)\n",
    "\n",
    "        style_loss = 0\n",
    "        content_loss = 0\n",
    "        for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "            # Compute content loss with target and content images\n",
    "            content_loss += torch.mean((f1 - f2)**2)\n",
    "\n",
    "            # Reshape convolutional feature maps\n",
    "            _, c, h, w = f1.size()\n",
    "            f1 = f1.view(c, h * w)\n",
    "            f3 = f3.view(c, h * w)\n",
    "\n",
    "            # Compute gram matrix\n",
    "            f1 = torch.mm(f1, f1.t())\n",
    "            f3 = torch.mm(f3, f3.t())\n",
    "\n",
    "            # Compute style loss with target and style images\n",
    "            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n",
    "        \n",
    "        # Compute total loss, backprop and optimize\n",
    "        loss = content_loss + config['style_weight'] * style_loss \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step+1) % config['log_step'] == 0:\n",
    "            print ('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}' \n",
    "                   .format(step+1, config['total_step'], content_loss.item(), style_loss.item()))\n",
    "\n",
    "        if (step+1) % config['sample_step'] == 0:\n",
    "            # Save the generated image\n",
    "            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "            img = target.clone().squeeze()\n",
    "            img = denorm(img).clamp_(0, 1)\n",
    "            torchvision.utils.save_image(img, os.path.join(config['output_dir'], 'output-{}.png'.format(step+1)))\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'content_path': '/Users/harshsingh/Desktop/Style transfer/Dataset/content_1.jpg',\n",
    "    'style_path': '/Users/harshsingh/Desktop/Style transfer/Dataset/MonaLisaStyle.jpg',\n",
    "    'output_dir': '/Users/harshsingh/Desktop/Style transfer',\n",
    "    'max_size': 400, \n",
    "    'total_step': 2000,\n",
    "    'log_step': 10,\n",
    "    'sample_step': 500,\n",
    "    'style_weight': 100,\n",
    "    'lr': 0.003\n",
    "}\n",
    "\n",
    "# Create output directory if not exists\n",
    "if not os.path.exists(config['output_dir']):\n",
    "    os.makedirs(config['output_dir'])\n",
    "\n",
    "# Run the main function\n",
    "main(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd277a-6d55-40cd-abfb-a922d7aa784b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9713366-cfe5-431c-b192-3880d0554562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816d625-265e-4005-835a-853edcf89f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8318857a-effe-431e-b614-e1f7125a0eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8ba0f-e1b0-4c17-9f5f-7be82a67ef96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d9bbc-5803-459d-9053-bd8c1b2aaf3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c467d4-1f92-4e3b-bd1a-07ffd93f051c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec100cfe-9bc4-401b-af0f-3ec063668368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d174f2-6c91-4403-b338-441298b37690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de5e5cf9-ef14-4373-995b-b6606b48d93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/5000], Content Loss: 1.2266, Style Loss: 7792.7632\n",
      "Step [20/5000], Content Loss: 5.9670, Style Loss: 7498.9302\n",
      "Step [30/5000], Content Loss: 13.1665, Style Loss: 7065.3926\n",
      "Step [40/5000], Content Loss: 20.7102, Style Loss: 6575.8906\n",
      "Step [50/5000], Content Loss: 27.0949, Style Loss: 6109.1709\n",
      "Step [60/5000], Content Loss: 31.9386, Style Loss: 5683.2451\n",
      "Step [70/5000], Content Loss: 35.6233, Style Loss: 5302.2534\n",
      "Step [80/5000], Content Loss: 38.5533, Style Loss: 4963.3115\n",
      "Step [90/5000], Content Loss: 40.9937, Style Loss: 4664.7139\n",
      "Step [100/5000], Content Loss: 43.1280, Style Loss: 4402.3823\n",
      "Step [110/5000], Content Loss: 45.0502, Style Loss: 4171.3662\n",
      "Step [120/5000], Content Loss: 46.8084, Style Loss: 3967.0830\n",
      "Step [130/5000], Content Loss: 48.4225, Style Loss: 3784.6677\n",
      "Step [140/5000], Content Loss: 49.9230, Style Loss: 3620.0852\n",
      "Step [150/5000], Content Loss: 51.3250, Style Loss: 3470.2720\n",
      "Step [160/5000], Content Loss: 52.6428, Style Loss: 3332.2625\n",
      "Step [170/5000], Content Loss: 53.8860, Style Loss: 3204.6016\n",
      "Step [180/5000], Content Loss: 55.0696, Style Loss: 3085.6709\n",
      "Step [190/5000], Content Loss: 56.2034, Style Loss: 2974.2256\n",
      "Step [200/5000], Content Loss: 57.2918, Style Loss: 2869.4795\n",
      "Step [210/5000], Content Loss: 58.3337, Style Loss: 2770.6780\n",
      "Step [220/5000], Content Loss: 59.3365, Style Loss: 2677.2446\n",
      "Step [230/5000], Content Loss: 60.2896, Style Loss: 2588.6323\n",
      "Step [240/5000], Content Loss: 61.2110, Style Loss: 2504.4277\n",
      "Step [250/5000], Content Loss: 62.1061, Style Loss: 2424.2725\n",
      "Step [260/5000], Content Loss: 62.9789, Style Loss: 2347.8789\n",
      "Step [270/5000], Content Loss: 63.8230, Style Loss: 2275.0337\n",
      "Step [280/5000], Content Loss: 64.6379, Style Loss: 2205.4980\n",
      "Step [290/5000], Content Loss: 65.4286, Style Loss: 2139.1562\n",
      "Step [300/5000], Content Loss: 66.2005, Style Loss: 2075.8870\n",
      "Step [310/5000], Content Loss: 66.9429, Style Loss: 2015.3595\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 125\u001b[0m\n\u001b[1;32m    122\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m main(config)\n",
      "Cell \u001b[0;32mIn[6], line 66\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_step\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     63\u001b[0m     \n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Extract multiple(5) conv feature vectors\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     target_features \u001b[38;5;241m=\u001b[39m vgg(target)\n\u001b[0;32m---> 66\u001b[0m     content_features \u001b[38;5;241m=\u001b[39m vgg(content)\n\u001b[1;32m     67\u001b[0m     style_features \u001b[38;5;241m=\u001b[39m vgg(style)\n\u001b[1;32m     69\u001b[0m     style_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m, in \u001b[0;36mVGGNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvgg\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 39\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect:\n\u001b[1;32m     41\u001b[0m         features\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
    "    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if max_size:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.LANCZOS)\n",
    "    \n",
    "    if shape:\n",
    "        image = image.resize(shape, Image.LANCZOS)\n",
    "    \n",
    "    if transform:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image.to(device)\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28'] \n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "def main(config):\n",
    "    # Image preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Load content and style images\n",
    "    # Make the style image the same size as the content image\n",
    "    content = load_image(config['content_path'], transform, max_size=config['max_size'])\n",
    "    style = load_image(config['style_path'], transform, shape=[content.size(2), content.size(3)])\n",
    "    \n",
    "    # Initialize a target image with the content image\n",
    "    target = content.clone().requires_grad_(True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([target], lr=config['lr'], betas=[0.5, 0.999])\n",
    "    vgg = VGGNet().to(device).eval()\n",
    "    \n",
    "    for step in range(config['total_step']):\n",
    "        \n",
    "        # Extract multiple(5) conv feature vectors\n",
    "        target_features = vgg(target)\n",
    "        content_features = vgg(content)\n",
    "        style_features = vgg(style)\n",
    "\n",
    "        style_loss = 0\n",
    "        content_loss = 0\n",
    "        for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "            # Compute content loss with target and content images\n",
    "            content_loss += torch.mean((f1 - f2)**2)\n",
    "\n",
    "            # Reshape convolutional feature maps\n",
    "            _, c, h, w = f1.size()\n",
    "            f1 = f1.view(c, h * w)\n",
    "            f3 = f3.view(c, h * w)\n",
    "\n",
    "            # Compute gram matrix\n",
    "            f1 = torch.mm(f1, f1.t())\n",
    "            f3 = torch.mm(f3, f3.t())\n",
    "\n",
    "            # Compute style loss with target and style images\n",
    "            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n",
    "        \n",
    "        # Compute total loss, backprop and optimize\n",
    "            #   loss = content_loss + config['style_weight'] * style_loss \n",
    "\n",
    "        loss = config['alpha'] * content_loss + config['beta'] * style_loss \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step+1) % config['log_step'] == 0:\n",
    "            print ('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}' \n",
    "                   .format(step+1, config['total_step'], content_loss.item(), style_loss.item()))\n",
    "\n",
    "        if (step+1) % config['sample_step'] == 0:\n",
    "            # Save the generated image\n",
    "            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "            img = target.clone().squeeze()\n",
    "            img = denorm(img).clamp_(0, 1)\n",
    "            torchvision.utils.save_image(img, os.path.join(config['output_dir'], 'output-{}.png'.format(step+1)))\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'content_path': '/Users/harshsingh/Desktop/Style transfer/Dataset/monaLisa.jpg',\n",
    "    'style_path': '/Users/harshsingh/Desktop/Style transfer/Dataset/heart_style.png',\n",
    "    'output_dir': '/Users/harshsingh/Desktop/Style transfer/mona_dil',\n",
    "    'max_size': 360, \n",
    "    'total_step': 5000,\n",
    "    'log_step': 10,\n",
    "    'sample_step': 250,\n",
    "    'style_weight': 100,\n",
    "    'alpha': 1,  # Content loss weight\n",
    "    'beta': 100, # Style loss weight\n",
    "    'lr': 0.001\n",
    "}\n",
    "# Create output directory if not exists\n",
    "if not os.path.exists(config['output_dir']):\n",
    "    os.makedirs(config['output_dir'])\n",
    "\n",
    "# Run the main function\n",
    "main(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de9b91-af44-48b3-8acb-7e289cb0446f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
